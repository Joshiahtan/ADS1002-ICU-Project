{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f10c087-4fe0-4e3c-b2be-3963eb9bad41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libraries and modules\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d064f02-e10b-432d-8af1-86f0cde90307",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Preprocessed ICU data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7b239c-95e9-4caa-9bf3-46b78d7f6302",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b96be1f-235f-42b3-a485-1d7b79d0807c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.        , 0.56192144])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Gender.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac3034c-557a-4155-a723-5b13d4de2ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~np.isclose(df[\"Gender\"].round(8), 0.56192144,atol=1e-8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a88ec4b-363f-445b-87a2-bb124e93c285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0.])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Gender.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1b61a8-8749-45f8-b3cb-aedd05d86879",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = df.iloc[:,:7]\n",
    "mean1 = df.iloc[:,7:44]\n",
    "min1 = df.iloc[:,44:81]\n",
    "max1 = df.iloc[:,81:118]\n",
    "mean2 = df.iloc[:,118:155]\n",
    "min2 = df.iloc[:,155:192]\n",
    "max2 = df.iloc[:,192:229]\n",
    "outcome = df.iloc[:,229:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd6651b-7ecf-441e-991a-3b173edd2c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0b14db-ac4c-4676-b757-d6445e940602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen = gen.drop(\"RecordID\", axis =1)\n",
    "first = pd.concat([mean1,max1,min1], axis = 1)\n",
    "\n",
    "rand_samp1 = mean1.sample(7, axis= 1, ignore_index = True, replace = False, random_state = 42)\n",
    "rand_samp2 = mean1.sample(7, axis= 1, ignore_index = True, replace = False, random_state = 4)\n",
    "rand_samp3 = mean1.sample(7, axis= 1, ignore_index = True, replace = False, random_state = 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c89859-bf09-47a0-8dce-f8796c198fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try1 = pd.concat([gen[[\"Age\", \"Height\"]],mean1], axis = 1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b6341952-ff73-4f53-abd2-e933e24dfb4a",
   "metadata": {},
   "source": [
    "print(rand_samp1.columns,rand_samp2.columns,rand_samp3.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68c0447-ae03-4e2d-9d33-806e1a8a8fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = rand_samp1 # input features\n",
    "y = outcome[\"In.hospital_death\"] # target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a51f631-b457-46a6-9bdd-ade570c01276",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9c169e-1903-4e9a-ab9e-fb76ae107189",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 3)\n",
    "X_pca = pca.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89ea4d2-7631-457d-a42c-aee588a340e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Explained variance: \", pca.explained_variance_ratio_)\n",
    "print(\"Cumulative: \", np.cumsum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7010f0bb-17ae-4c6a-a17e-0661299c0055",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reconstructed = pca.inverse_transform(X_pca)\n",
    "reconstruction_loss = np.mean((X_scaled - X_reconstructed) ** 2)\n",
    "print(f\"Reconstruction Loss: {reconstruction_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e15f3c-f973-4bcd-b115-66225c4fa8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to be able to sample particular datasets, and display the results of the dataframe\n",
    "\n",
    "def display_pca(dataset,n, size, components): # given a dataset, n is the number of random samples, s is the sample size\n",
    "    # results = pd.DataFrame()\n",
    "    # seeds = list(range(1,n+1))\n",
    "    mean_variance = 0\n",
    "    mean_reconstruction_loss = 0\n",
    "    for i in range(n):\n",
    "        rand_samp = dataset.sample(size, axis = 1, ignore_index = True, replace = False, random_state = i)\n",
    "        X = rand_samp\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        pca = PCA(n_components = components)\n",
    "        X_pca = pca.fit_transform(X_scaled)\n",
    "        X_reconstructed = pca.inverse_transform(X_pca)\n",
    "        reconstruction_loss = np.mean((X_scaled - X_reconstructed) ** 2)\n",
    "        print(f\"Explained variance: {pca.explained_variance_ratio_}\\nCumulative: {np.cumsum(pca.explained_variance_ratio_)}\\nTrial no.: {i+1}\\nReconstruction Loss: {reconstruction_loss:.4f}\\n\")\n",
    "        mean_variance = (mean_variance*i+np.cumsum(pca.explained_variance_ratio_)[-1])/(i+1)\n",
    "        mean_reconstruction_loss = (mean_reconstruction_loss*i+reconstruction_loss)/(i+1)\n",
    "    print(f\"Mean variance: {mean_variance}\\nMean reconstruction loss: {mean_reconstruction_loss}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464a8eaf-20d4-4343-8830-b0440cd63db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_pca(mean1,10,8,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb178d04-210b-4390-a514-12fe60cdab03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_pca2(dataset: pd.DataFrame,n: int, size: int, components: int) -> pd.DataFrame: # given a dataset, n is the number of random samples, s is the sample size\n",
    "    \"\"\"\n",
    "    This\n",
    "    \"\"\"\n",
    "    results = pd.DataFrame(columns=[\"Trial no.\", \"Cumulative variance\", \"Reconstruction Loss\", \"Features chosen\"])\n",
    "    mean_variance = 0\n",
    "    mean_reconstruction_loss = 0\n",
    "    for i in range(n):\n",
    "        rand_samp = dataset.sample(size, axis = 1, ignore_index = True, replace = False, random_state = i)\n",
    "        cols = rand_samp.columns.tolist()\n",
    "        X = rand_samp\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        pca = PCA(n_components = components)\n",
    "        X_pca = pca.fit_transform(X_scaled)\n",
    "        X_reconstructed = pca.inverse_transform(X_pca)\n",
    "        reconstruction_loss = np.mean((X_scaled - X_reconstructed) ** 2)\n",
    "        cum_var = np.cumsum(pca.explained_variance_ratio_)[-1]\n",
    "        new_row = pd.DataFrame({\"Trial no.\": [i+1], \"Cumulative variance\": [cum_var], \"Reconstruction Loss\": [reconstruction_loss], \"Features chosen\": [cols]})\n",
    "        results = pd.concat([results, new_row], axis = 0, ignore_index = True)\n",
    "       \n",
    "\n",
    "        \n",
    "        mean_variance = (mean_variance*i+cum_var)/(i+1)\n",
    "        mean_reconstruction_loss = (mean_reconstruction_loss*i+reconstruction_loss)/(i+1)\n",
    "\n",
    "    results = pd.concat([results, pd.DataFrame({\"Trial no.\": [\"Means\"], \"Cumulative variance\": [mean_variance], \"Reconstruction Loss\": [mean_reconstruction_loss]})], axis = 0, ignore_index = True)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9751ebfb-cb1e-49e5-a22b-6599b2db868c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results2 = display_pca2(mean1,20,6,3)\n",
    "results3 = display_pca2(mean2,20,6,3)\n",
    "results4 = display_pca2(min1,20,6,3)\n",
    "results5 = display_pca2(min2,20,6,3)\n",
    "results6 = display_pca2(max1,20,6,3)\n",
    "results7 = display_pca2(max2,20,6,3)\n",
    "results8 = display_pca2(first,50,6,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1e9962-9290-429c-9017-0ad122cbca21",
   "metadata": {},
   "outputs": [],
   "source": [
    "results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afca3fba-6a59-4d94-bce0-af3e5e62cdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255cf114-f464-4f17-9422-aa075aa847da",
   "metadata": {},
   "outputs": [],
   "source": [
    "results4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d80c7d-7e7a-4c42-b4e6-738a0d8712f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3366498c-f3eb-4ac3-b26a-afc8e01cd826",
   "metadata": {},
   "outputs": [],
   "source": [
    "results6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5932f2-93cc-44ff-b88a-eec30f785845",
   "metadata": {},
   "outputs": [],
   "source": [
    "results7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5fef73-07ae-43f5-90d3-2946813a0bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "results8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b5ba4f-dc5f-4fa8-b978-5595e7c5e632",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eac58a-b072-4e14-ad4a-6c648ff03fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X = mean1\n",
    "\n",
    "rfe = RFE(estimator = RandomForestClassifier(), n_features_to_select = 5)\n",
    "model = RandomForestClassifier()\n",
    "pipeline = Pipeline(steps=[(\"Feature Selection\", rfe), (\"Model\", model)])\n",
    "\n",
    "\n",
    "# evaluating the model\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits = 10, n_repeats = 5, random_state=1)\n",
    "\n",
    "results = cross_validate(pipeline, X, y, scoring=\"accuracy\", cv=cv, return_estimator = True)\n",
    "n_scores = cross_validate(pipeline, X,y, scoring=\"accuracy\", cv=cv, n_jobs=1)\n",
    "\n",
    "print(f\"Accuracy: {(n_scores)} {(n_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9e04dc7e-a33a-4914-af5d-bdc9db60c8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[199  69  18]\n",
      "  [143  88 159]\n",
      "  [201  68 186]\n",
      "  [112 187 162]]\n",
      "\n",
      " [[181  36 173]\n",
      "  [ 39 193 190]\n",
      "  [238  18  12]\n",
      "  [ 51 169 123]]\n",
      "\n",
      " [[138 106  54]\n",
      "  [147 236  42]\n",
      "  [178  53 202]\n",
      "  [ 89 140  90]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a 3x4 array with random integers between 0 and 255\n",
    "array = np.random.randint(0, 256, size=(3, 4, 3))\n",
    "print(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d34724c-d5e1-44ce-b06d-38f28625dd89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
